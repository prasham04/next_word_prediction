{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12efa4b9",
   "metadata": {
    "id": "12efa4b9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Read the text file\n",
    "with open('sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f395c88",
   "metadata": {
    "id": "9f395c88"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2b818ec-5a0d-4fc0-9400-3143fb47f7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'and': 2,\n",
       " 'i': 3,\n",
       " 'to': 4,\n",
       " 'of': 5,\n",
       " 'a': 6,\n",
       " 'in': 7,\n",
       " 'that': 8,\n",
       " 'it': 9,\n",
       " 'he': 10,\n",
       " 'you': 11,\n",
       " 'was': 12,\n",
       " 'his': 13,\n",
       " 'is': 14,\n",
       " 'my': 15,\n",
       " 'have': 16,\n",
       " 'as': 17,\n",
       " 'with': 18,\n",
       " 'had': 19,\n",
       " 'which': 20,\n",
       " 'at': 21,\n",
       " 'for': 22,\n",
       " 'but': 23,\n",
       " 'me': 24,\n",
       " 'not': 25,\n",
       " 'be': 26,\n",
       " 'we': 27,\n",
       " 'from': 28,\n",
       " 'there': 29,\n",
       " 'this': 30,\n",
       " 'said': 31,\n",
       " 'upon': 32,\n",
       " 'so': 33,\n",
       " 'holmes': 34,\n",
       " 'him': 35,\n",
       " 'her': 36,\n",
       " 'she': 37,\n",
       " \"'\": 38,\n",
       " 'very': 39,\n",
       " 'your': 40,\n",
       " 'been': 41,\n",
       " 'all': 42,\n",
       " 'on': 43,\n",
       " 'no': 44,\n",
       " 'what': 45,\n",
       " 'one': 46,\n",
       " 'then': 47,\n",
       " 'were': 48,\n",
       " 'by': 49,\n",
       " 'are': 50,\n",
       " 'an': 51,\n",
       " 'would': 52,\n",
       " 'out': 53,\n",
       " 'when': 54,\n",
       " 'up': 55,\n",
       " 'man': 56,\n",
       " 'could': 57,\n",
       " 'has': 58,\n",
       " 'do': 59,\n",
       " 'into': 60,\n",
       " 'mr': 61,\n",
       " 'who': 62,\n",
       " 'little': 63,\n",
       " 'will': 64,\n",
       " 'if': 65,\n",
       " 'some': 66,\n",
       " 'now': 67,\n",
       " 'see': 68,\n",
       " 'down': 69,\n",
       " 'should': 70,\n",
       " 'our': 71,\n",
       " 'or': 72,\n",
       " 'they': 73,\n",
       " 'may': 74,\n",
       " 'well': 75,\n",
       " 'am': 76,\n",
       " 'us': 77,\n",
       " 'over': 78,\n",
       " 'more': 79,\n",
       " 'think': 80,\n",
       " 'room': 81,\n",
       " 'know': 82,\n",
       " 'shall': 83,\n",
       " 'about': 84,\n",
       " 'can': 85,\n",
       " 'before': 86,\n",
       " 'must': 87,\n",
       " 'only': 88,\n",
       " 'come': 89,\n",
       " 'than': 90,\n",
       " 'did': 91,\n",
       " 'time': 92,\n",
       " 'two': 93,\n",
       " 'other': 94,\n",
       " 'came': 95,\n",
       " 'them': 96,\n",
       " 'door': 97,\n",
       " 'back': 98,\n",
       " 'how': 99,\n",
       " 'good': 100,\n",
       " 'here': 101,\n",
       " 'any': 102,\n",
       " 'face': 103,\n",
       " 'might': 104,\n",
       " 'just': 105,\n",
       " 'matter': 106,\n",
       " 'house': 107,\n",
       " 'much': 108,\n",
       " 'hand': 109,\n",
       " 'case': 110,\n",
       " 'way': 111,\n",
       " 'where': 112,\n",
       " 'night': 113,\n",
       " 'yes': 114,\n",
       " 'heard': 115,\n",
       " 'such': 116,\n",
       " 'made': 117,\n",
       " 'nothing': 118,\n",
       " 'however': 119,\n",
       " 'found': 120,\n",
       " 'away': 121,\n",
       " 'day': 122,\n",
       " 'never': 123,\n",
       " 'quite': 124,\n",
       " 'morning': 125,\n",
       " 'own': 126,\n",
       " 'go': 127,\n",
       " 'after': 128,\n",
       " 'sherlock': 129,\n",
       " 'right': 130,\n",
       " 'their': 131,\n",
       " 'like': 132,\n",
       " 'tell': 133,\n",
       " 'last': 134,\n",
       " 'say': 135,\n",
       " 'left': 136,\n",
       " 'through': 137,\n",
       " 'saw': 138,\n",
       " 'most': 139,\n",
       " 'yet': 140,\n",
       " 'side': 141,\n",
       " 'asked': 142,\n",
       " 'eyes': 143,\n",
       " 'long': 144,\n",
       " 'took': 145,\n",
       " 'miss': 146,\n",
       " 'its': 147,\n",
       " 'once': 148,\n",
       " 'first': 149,\n",
       " 'street': 150,\n",
       " 'too': 151,\n",
       " 'every': 152,\n",
       " 'watson': 153,\n",
       " 'round': 154,\n",
       " 'young': 155,\n",
       " 'st': 156,\n",
       " 'still': 157,\n",
       " 'these': 158,\n",
       " 'find': 159,\n",
       " 'take': 160,\n",
       " 'small': 161,\n",
       " 'thought': 162,\n",
       " 'myself': 163,\n",
       " 'sir': 164,\n",
       " 'few': 165,\n",
       " 'light': 166,\n",
       " 'oh': 167,\n",
       " 'until': 168,\n",
       " 'off': 169,\n",
       " 'without': 170,\n",
       " 'himself': 171,\n",
       " 'hands': 172,\n",
       " 'make': 173,\n",
       " 'business': 174,\n",
       " 'father': 175,\n",
       " 'seen': 176,\n",
       " 'old': 177,\n",
       " 'window': 178,\n",
       " 'lady': 179,\n",
       " 'look': 180,\n",
       " 'three': 181,\n",
       " 'ever': 182,\n",
       " 'even': 183,\n",
       " 'friend': 184,\n",
       " 'let': 185,\n",
       " 'cried': 186,\n",
       " 'seemed': 187,\n",
       " 'again': 188,\n",
       " 'head': 189,\n",
       " 'went': 190,\n",
       " 'having': 191,\n",
       " 'put': 192,\n",
       " 'why': 193,\n",
       " 'done': 194,\n",
       " 'while': 195,\n",
       " 'those': 196,\n",
       " 'something': 197,\n",
       " 'doubt': 198,\n",
       " 'remarked': 199,\n",
       " 'open': 200,\n",
       " 'rather': 201,\n",
       " 'years': 202,\n",
       " 'though': 203,\n",
       " 'name': 204,\n",
       " 'indeed': 205,\n",
       " 'chair': 206,\n",
       " 'half': 207,\n",
       " 'perhaps': 208,\n",
       " 'get': 209,\n",
       " 'woman': 210,\n",
       " 'between': 211,\n",
       " 'give': 212,\n",
       " 'great': 213,\n",
       " 'course': 214,\n",
       " 'always': 215,\n",
       " 'mind': 216,\n",
       " 'enough': 217,\n",
       " 'knew': 218,\n",
       " 'end': 219,\n",
       " 'answered': 220,\n",
       " 'same': 221,\n",
       " 'far': 222,\n",
       " 'sat': 223,\n",
       " 'looking': 224,\n",
       " 'place': 225,\n",
       " 'table': 226,\n",
       " 'dear': 227,\n",
       " 'wife': 228,\n",
       " 'got': 229,\n",
       " 'police': 230,\n",
       " 'against': 231,\n",
       " 'also': 232,\n",
       " 'really': 233,\n",
       " 'red': 234,\n",
       " 'looked': 235,\n",
       " 'black': 236,\n",
       " 'better': 237,\n",
       " 'anything': 238,\n",
       " 'turned': 239,\n",
       " 'cannot': 240,\n",
       " 'behind': 241,\n",
       " 'told': 242,\n",
       " 'hardly': 243,\n",
       " 'hat': 244,\n",
       " 'front': 245,\n",
       " 'brought': 246,\n",
       " 'possible': 247,\n",
       " 'home': 248,\n",
       " 'understand': 249,\n",
       " 'life': 250,\n",
       " 'leave': 251,\n",
       " 'within': 252,\n",
       " 'work': 253,\n",
       " 'help': 254,\n",
       " 'thing': 255,\n",
       " 'both': 256,\n",
       " 'already': 257,\n",
       " 'suddenly': 258,\n",
       " 'strange': 259,\n",
       " 'gave': 260,\n",
       " 'words': 261,\n",
       " 'son': 262,\n",
       " 'whole': 263,\n",
       " 'fire': 264,\n",
       " 'point': 265,\n",
       " 'paper': 266,\n",
       " 'being': 267,\n",
       " 'papers': 268,\n",
       " 'minutes': 269,\n",
       " 'hair': 270,\n",
       " 'clear': 271,\n",
       " 'money': 272,\n",
       " 'wish': 273,\n",
       " 'gone': 274,\n",
       " 'under': 275,\n",
       " 'call': 276,\n",
       " 'sure': 277,\n",
       " 'whether': 278,\n",
       " \"'i\": 279,\n",
       " 'mrs': 280,\n",
       " 'set': 281,\n",
       " 'yourself': 282,\n",
       " 'certainly': 283,\n",
       " 'gentleman': 284,\n",
       " 'many': 285,\n",
       " 'pray': 286,\n",
       " 'another': 287,\n",
       " 'lay': 288,\n",
       " 'baker': 289,\n",
       " 'passed': 290,\n",
       " 'london': 291,\n",
       " 'large': 292,\n",
       " 'days': 293,\n",
       " 'five': 294,\n",
       " 'dark': 295,\n",
       " 'word': 296,\n",
       " 'met': 297,\n",
       " 'since': 298,\n",
       " 'whom': 299,\n",
       " 'does': 300,\n",
       " 'soon': 301,\n",
       " 'lord': 302,\n",
       " 'bed': 303,\n",
       " 'simon': 304,\n",
       " 'less': 305,\n",
       " 'during': 306,\n",
       " 'men': 307,\n",
       " 'four': 308,\n",
       " 'interest': 309,\n",
       " 'evening': 310,\n",
       " 'save': 311,\n",
       " 'across': 312,\n",
       " 'note': 313,\n",
       " 'lestrade': 314,\n",
       " 'returned': 315,\n",
       " 'read': 316,\n",
       " 'opened': 317,\n",
       " 'least': 318,\n",
       " 'strong': 319,\n",
       " 'among': 320,\n",
       " 'story': 321,\n",
       " 'stood': 322,\n",
       " 'country': 323,\n",
       " 'together': 324,\n",
       " 'believe': 325,\n",
       " 'facts': 326,\n",
       " 'doctor': 327,\n",
       " 'question': 328,\n",
       " 'ask': 329,\n",
       " 'moment': 330,\n",
       " 'none': 331,\n",
       " 'waiting': 332,\n",
       " 'fellow': 333,\n",
       " 'rucastle': 334,\n",
       " 'felt': 335,\n",
       " 'either': 336,\n",
       " \"o'clock\": 337,\n",
       " 'entered': 338,\n",
       " 'sitting': 339,\n",
       " 'use': 340,\n",
       " 'rushed': 341,\n",
       " 'mccarthy': 342,\n",
       " 'each': 343,\n",
       " 'crime': 344,\n",
       " 'singular': 345,\n",
       " 'part': 346,\n",
       " 'corner': 347,\n",
       " 'seven': 348,\n",
       " 'given': 349,\n",
       " 'hear': 350,\n",
       " 'hour': 351,\n",
       " 'else': 352,\n",
       " 'laid': 353,\n",
       " 'road': 354,\n",
       " 'going': 355,\n",
       " 'sound': 356,\n",
       " 'client': 357,\n",
       " 'best': 358,\n",
       " 'able': 359,\n",
       " 'became': 360,\n",
       " 'near': 361,\n",
       " 'instant': 362,\n",
       " 'hope': 363,\n",
       " 'forward': 364,\n",
       " 'used': 365,\n",
       " \"'you\": 366,\n",
       " 'stone': 367,\n",
       " 'family': 368,\n",
       " 'companion': 369,\n",
       " 'new': 370,\n",
       " 'heavy': 371,\n",
       " \"it's\": 372,\n",
       " 'turn': 373,\n",
       " 'several': 374,\n",
       " 'letter': 375,\n",
       " 'ran': 376,\n",
       " 'cut': 377,\n",
       " 'rooms': 378,\n",
       " 'threw': 379,\n",
       " 'true': 380,\n",
       " 'imagine': 381,\n",
       " 'six': 382,\n",
       " \"don't\": 383,\n",
       " 'past': 384,\n",
       " 'dr': 385,\n",
       " 'white': 386,\n",
       " 'known': 387,\n",
       " 'year': 388,\n",
       " 'floor': 389,\n",
       " 'hard': 390,\n",
       " 'ten': 391,\n",
       " 'obvious': 392,\n",
       " 'inspector': 393,\n",
       " 'coronet': 394,\n",
       " 'late': 395,\n",
       " 'manner': 396,\n",
       " 'ago': 397,\n",
       " 'death': 398,\n",
       " 'want': 399,\n",
       " 'feet': 400,\n",
       " 'coat': 401,\n",
       " 'taken': 402,\n",
       " 'station': 403,\n",
       " 'walked': 404,\n",
       " 'fear': 405,\n",
       " 'seems': 406,\n",
       " 'cry': 407,\n",
       " 'coming': 408,\n",
       " 'ah': 409,\n",
       " 'lamp': 410,\n",
       " 'spoke': 411,\n",
       " 'things': 412,\n",
       " 'marriage': 413,\n",
       " 'lost': 414,\n",
       " 'dress': 415,\n",
       " 'appeared': 416,\n",
       " 'present': 417,\n",
       " 'absolutely': 418,\n",
       " 'air': 419,\n",
       " 'above': 420,\n",
       " 'goose': 421,\n",
       " 'colonel': 422,\n",
       " 'blue': 423,\n",
       " 'visitor': 424,\n",
       " 'deep': 425,\n",
       " 'alone': 426,\n",
       " 'rose': 427,\n",
       " 'called': 428,\n",
       " 'letters': 429,\n",
       " 'cab': 430,\n",
       " 'drove': 431,\n",
       " 'god': 432,\n",
       " 'dressed': 433,\n",
       " 'remember': 434,\n",
       " 'mine': 435,\n",
       " 'reason': 436,\n",
       " 'keep': 437,\n",
       " 'held': 438,\n",
       " 'office': 439,\n",
       " 'sister': 440,\n",
       " 'week': 441,\n",
       " 'led': 442,\n",
       " 'bell': 443,\n",
       " 'steps': 444,\n",
       " 'address': 445,\n",
       " 'followed': 446,\n",
       " 'king': 447,\n",
       " 'photograph': 448,\n",
       " 'married': 449,\n",
       " 'second': 450,\n",
       " 'beside': 451,\n",
       " 'quick': 452,\n",
       " 'windows': 453,\n",
       " 'people': 454,\n",
       " 'began': 455,\n",
       " 'john': 456,\n",
       " 'chance': 457,\n",
       " 'easy': 458,\n",
       " 'heart': 459,\n",
       " 'fact': 460,\n",
       " 'square': 461,\n",
       " 'headed': 462,\n",
       " 'nature': 463,\n",
       " 'attention': 464,\n",
       " 'eye': 465,\n",
       " 'step': 466,\n",
       " 'outside': 467,\n",
       " 'cases': 468,\n",
       " 'likely': 469,\n",
       " 'twenty': 470,\n",
       " 'started': 471,\n",
       " 'anyone': 472,\n",
       " 'struck': 473,\n",
       " 'silence': 474,\n",
       " 'thank': 475,\n",
       " 'grey': 476,\n",
       " 'ready': 477,\n",
       " 'girl': 478,\n",
       " 'glancing': 479,\n",
       " 'passage': 480,\n",
       " 'sprang': 481,\n",
       " 'ground': 482,\n",
       " 'daughter': 483,\n",
       " 'bring': 484,\n",
       " 'next': 485,\n",
       " 'lane': 486,\n",
       " 'whose': 487,\n",
       " 'maid': 488,\n",
       " 'pocket': 489,\n",
       " 'standing': 490,\n",
       " 'sort': 491,\n",
       " 'poor': 492,\n",
       " 'town': 493,\n",
       " 'happened': 494,\n",
       " 'idea': 495,\n",
       " 'breakfast': 496,\n",
       " 'holder': 497,\n",
       " 'clair': 498,\n",
       " 'mystery': 499,\n",
       " 'cold': 500,\n",
       " 'position': 501,\n",
       " 'high': 502,\n",
       " 'clothes': 503,\n",
       " 'observed': 504,\n",
       " 'because': 505,\n",
       " 'carried': 506,\n",
       " 'person': 507,\n",
       " 'glanced': 508,\n",
       " 'hurried': 509,\n",
       " 'carriage': 510,\n",
       " 'drawn': 511,\n",
       " 'husband': 512,\n",
       " 'kind': 513,\n",
       " 'closed': 514,\n",
       " 'short': 515,\n",
       " 'bird': 516,\n",
       " 'hosmer': 517,\n",
       " 'occurred': 518,\n",
       " 'seeing': 519,\n",
       " 'mary': 520,\n",
       " 'order': 521,\n",
       " 'ha': 522,\n",
       " 'interesting': 523,\n",
       " 'doing': 524,\n",
       " 'important': 525,\n",
       " 'quiet': 526,\n",
       " 'need': 527,\n",
       " 'train': 528,\n",
       " 'afraid': 529,\n",
       " 'considerable': 530,\n",
       " 'body': 531,\n",
       " 'city': 532,\n",
       " 'sight': 533,\n",
       " 'points': 534,\n",
       " 'thin': 535,\n",
       " 'danger': 536,\n",
       " 'wedding': 537,\n",
       " 'public': 538,\n",
       " 'hunter': 539,\n",
       " 'account': 540,\n",
       " 'problem': 541,\n",
       " 'shown': 542,\n",
       " 'fashion': 543,\n",
       " 'matters': 544,\n",
       " 'written': 545,\n",
       " 'peculiar': 546,\n",
       " 'boy': 547,\n",
       " 'england': 548,\n",
       " 'character': 549,\n",
       " 'voice': 550,\n",
       " 'caught': 551,\n",
       " 'speak': 552,\n",
       " 'secret': 553,\n",
       " 'laughed': 554,\n",
       " 'close': 555,\n",
       " 'remarkable': 556,\n",
       " 'quietly': 557,\n",
       " 'towards': 558,\n",
       " 'mean': 559,\n",
       " 'drive': 560,\n",
       " 'cause': 561,\n",
       " 'turner': 562,\n",
       " 'hours': 563,\n",
       " 'fresh': 564,\n",
       " 'ourselves': 565,\n",
       " 'showed': 566,\n",
       " 'safe': 567,\n",
       " 'wilson': 568,\n",
       " 'advertisement': 569,\n",
       " 'earth': 570,\n",
       " 'experience': 571,\n",
       " 'everything': 572,\n",
       " 'opinion': 573,\n",
       " 'talk': 574,\n",
       " 'windibank': 575,\n",
       " 'mother': 576,\n",
       " 'adventure': 577,\n",
       " 'placed': 578,\n",
       " 'excellent': 579,\n",
       " 'extraordinary': 580,\n",
       " 'finally': 581,\n",
       " 'figure': 582,\n",
       " 'observe': 583,\n",
       " 'someone': 584,\n",
       " 'simple': 585,\n",
       " 'hall': 586,\n",
       " 'means': 587,\n",
       " 'sent': 588,\n",
       " 'hundred': 589,\n",
       " 'single': 590,\n",
       " 'slowly': 591,\n",
       " 'sign': 592,\n",
       " 'line': 593,\n",
       " 'lodge': 594,\n",
       " 'suppose': 595,\n",
       " 'child': 596,\n",
       " 'reached': 597,\n",
       " 'details': 598,\n",
       " 'wait': 599,\n",
       " 'others': 600,\n",
       " 'direction': 601,\n",
       " 'dead': 602,\n",
       " 'box': 603,\n",
       " 'return': 604,\n",
       " 'rest': 605,\n",
       " 'finger': 606,\n",
       " 'court': 607,\n",
       " 'effect': 608,\n",
       " 'colour': 609,\n",
       " 'advice': 610,\n",
       " 'pipe': 611,\n",
       " 'silent': 612,\n",
       " 'angel': 613,\n",
       " 'stepfather': 614,\n",
       " 'pool': 615,\n",
       " 'frank': 616,\n",
       " 'k': 617,\n",
       " 'neville': 618,\n",
       " 'stoner': 619,\n",
       " 'love': 620,\n",
       " 'remained': 621,\n",
       " 'glad': 622,\n",
       " 'itself': 623,\n",
       " 'almost': 624,\n",
       " 'laughing': 625,\n",
       " 'times': 626,\n",
       " 'serious': 627,\n",
       " 'gold': 628,\n",
       " 'news': 629,\n",
       " 'bedroom': 630,\n",
       " 'garden': 631,\n",
       " 'listened': 632,\n",
       " \"'and\": 633,\n",
       " 'nor': 634,\n",
       " 'afterwards': 635,\n",
       " 'entirely': 636,\n",
       " 'fell': 637,\n",
       " 'lips': 638,\n",
       " 'bright': 639,\n",
       " 'water': 640,\n",
       " 'low': 641,\n",
       " 'locked': 642,\n",
       " 'happy': 643,\n",
       " 'along': 644,\n",
       " \"i'll\": 645,\n",
       " 'james': 646,\n",
       " 'arthur': 647,\n",
       " 'league': 648,\n",
       " 'thumb': 649,\n",
       " 'dreadful': 650,\n",
       " 'appears': 651,\n",
       " 'glance': 652,\n",
       " 'sit': 653,\n",
       " 'boots': 654,\n",
       " 'pay': 655,\n",
       " 'morrow': 656,\n",
       " 'afternoon': 657,\n",
       " 'pulled': 658,\n",
       " \"won't\": 659,\n",
       " 'meet': 660,\n",
       " \"holmes'\": 661,\n",
       " 'drew': 662,\n",
       " 'making': 663,\n",
       " 'ring': 664,\n",
       " 'seem': 665,\n",
       " 'weeks': 666,\n",
       " 'foot': 667,\n",
       " 'wooden': 668,\n",
       " 'instantly': 669,\n",
       " 'surprised': 670,\n",
       " 'feel': 671,\n",
       " 'hatherley': 672,\n",
       " 'evidence': 673,\n",
       " 'miles': 674,\n",
       " 'innocent': 675,\n",
       " 'feeling': 676,\n",
       " 'geese': 677,\n",
       " 'boscombe': 678,\n",
       " 'irene': 679,\n",
       " 'adler': 680,\n",
       " 'machine': 681,\n",
       " 'lit': 682,\n",
       " 'twice': 683,\n",
       " 'chamber': 684,\n",
       " 'kindly': 685,\n",
       " 'double': 686,\n",
       " 'show': 687,\n",
       " 'explain': 688,\n",
       " 'yours': 689,\n",
       " 'importance': 690,\n",
       " 'examined': 691,\n",
       " 'brown': 692,\n",
       " 'comes': 693,\n",
       " 'sharp': 694,\n",
       " 'shoulders': 695,\n",
       " 'impression': 696,\n",
       " 'appearance': 697,\n",
       " 'broad': 698,\n",
       " 'trust': 699,\n",
       " 'confess': 700,\n",
       " 'surprise': 701,\n",
       " 'majesty': 702,\n",
       " 'mad': 703,\n",
       " 'lock': 704,\n",
       " 'arrived': 705,\n",
       " 'dropped': 706,\n",
       " 'whispered': 707,\n",
       " 'taking': 708,\n",
       " 'centre': 709,\n",
       " 'determined': 710,\n",
       " 'change': 711,\n",
       " 'key': 712,\n",
       " 'turning': 713,\n",
       " 'company': 714,\n",
       " 'events': 715,\n",
       " 'shook': 716,\n",
       " 'full': 717,\n",
       " 'third': 718,\n",
       " 'smiling': 719,\n",
       " 'yard': 720,\n",
       " 'dog': 721,\n",
       " 'sudden': 722,\n",
       " 'pale': 723,\n",
       " 'cleared': 724,\n",
       " 'dressing': 725,\n",
       " 'world': 726,\n",
       " 'armchair': 727,\n",
       " 'fancy': 728,\n",
       " 'walk': 729,\n",
       " 'caused': 730,\n",
       " 'german': 731,\n",
       " 'therefore': 732,\n",
       " 'excuse': 733,\n",
       " 'thirty': 734,\n",
       " 'certain': 735,\n",
       " 'view': 736,\n",
       " 'deal': 737,\n",
       " 'object': 738,\n",
       " 'probably': 739,\n",
       " 'witness': 740,\n",
       " 'whatever': 741,\n",
       " 'neither': 742,\n",
       " 'blood': 743,\n",
       " 'waited': 744,\n",
       " 'later': 745,\n",
       " 'engaged': 746,\n",
       " 'care': 747,\n",
       " 'slight': 748,\n",
       " 'cellar': 749,\n",
       " 'live': 750,\n",
       " 'says': 751,\n",
       " \"didn't\": 752,\n",
       " 'answer': 753,\n",
       " 'claim': 754,\n",
       " 'force': 755,\n",
       " 'yellow': 756,\n",
       " 'wrong': 757,\n",
       " 'coroner': 758,\n",
       " 'terrible': 759,\n",
       " 'truth': 760,\n",
       " 'openshaw': 761,\n",
       " 'toller': 762,\n",
       " 'band': 763,\n",
       " 'noble': 764,\n",
       " 'society': 765,\n",
       " 'pass': 766,\n",
       " 'swiftly': 767,\n",
       " 'top': 768,\n",
       " 'thick': 769,\n",
       " 'lying': 770,\n",
       " 'eight': 771,\n",
       " 'received': 772,\n",
       " 'wrote': 773,\n",
       " 'scene': 774,\n",
       " 'glass': 775,\n",
       " 'continued': 776,\n",
       " 'stairs': 777,\n",
       " 'bad': 778,\n",
       " 'pushed': 779,\n",
       " 'absolute': 780,\n",
       " 'promise': 781,\n",
       " 'difficult': 782,\n",
       " 'follow': 783,\n",
       " 'private': 784,\n",
       " 'result': 785,\n",
       " 'monday': 786,\n",
       " 'features': 787,\n",
       " 'wall': 788,\n",
       " 'evidently': 789,\n",
       " 'arms': 790,\n",
       " 'church': 791,\n",
       " 'shot': 792,\n",
       " \"'the\": 793,\n",
       " 'run': 794,\n",
       " 'smoke': 795,\n",
       " 'broke': 796,\n",
       " 'shoulder': 797,\n",
       " 'walking': 798,\n",
       " 'impossible': 799,\n",
       " 'age': 800,\n",
       " 'affair': 801,\n",
       " 'assistant': 802,\n",
       " 'common': 803,\n",
       " \"'oh\": 804,\n",
       " 'died': 805,\n",
       " 'clay': 806,\n",
       " 'sum': 807,\n",
       " 'bank': 808,\n",
       " 'amid': 809,\n",
       " 'early': 810,\n",
       " 'clearly': 811,\n",
       " 'nine': 812,\n",
       " 'presence': 813,\n",
       " 'darkness': 814,\n",
       " \"man's\": 815,\n",
       " 'reading': 816,\n",
       " 'uncle': 817,\n",
       " 'friends': 818,\n",
       " 'hotel': 819,\n",
       " 'professional': 820,\n",
       " 'frightened': 821,\n",
       " 'bent': 822,\n",
       " 'envelope': 823,\n",
       " 'den': 824,\n",
       " 'roylott': 825,\n",
       " 'ventilator': 826,\n",
       " 'scandal': 827,\n",
       " 'copper': 828,\n",
       " 'power': 829,\n",
       " 'master': 830,\n",
       " 'signs': 831,\n",
       " 'spoken': 832,\n",
       " 'deduce': 833,\n",
       " 'inside': 834,\n",
       " 'throwing': 835,\n",
       " 'example': 836,\n",
       " 'houses': 837,\n",
       " 'tried': 838,\n",
       " 'send': 839,\n",
       " 'wood': 840,\n",
       " 'situation': 841,\n",
       " 'expected': 842,\n",
       " 'faced': 843,\n",
       " 'running': 844,\n",
       " 'usual': 845,\n",
       " 'different': 846,\n",
       " 'nearly': 847,\n",
       " 'raise': 848,\n",
       " 'self': 849,\n",
       " 'besides': 850,\n",
       " \"he's\": 851,\n",
       " 'yesterday': 852,\n",
       " 'months': 853,\n",
       " 'slipped': 854,\n",
       " 'heavily': 855,\n",
       " 'ross': 856,\n",
       " 'building': 857,\n",
       " 'middle': 858,\n",
       " 'knowledge': 859,\n",
       " 'edge': 860,\n",
       " 'clue': 861,\n",
       " 'start': 862,\n",
       " 'missing': 863,\n",
       " 'trap': 864,\n",
       " 'moran': 865,\n",
       " 'charge': 866,\n",
       " 'break': 867,\n",
       " 'horrible': 868,\n",
       " 'horner': 869,\n",
       " 'gems': 870,\n",
       " 'bohemia': 871,\n",
       " 'pips': 872,\n",
       " 'lip': 873,\n",
       " 'beeches': 874,\n",
       " 'results': 875,\n",
       " 'complete': 876,\n",
       " 'keen': 877,\n",
       " 'deeply': 878,\n",
       " 'following': 879,\n",
       " 'official': 880,\n",
       " 'practice': 881,\n",
       " 'tall': 882,\n",
       " 'lived': 883,\n",
       " \"can't\": 884,\n",
       " 'often': 885,\n",
       " 'interested': 886,\n",
       " 'sheet': 887,\n",
       " 'writing': 888,\n",
       " 'precisely': 889,\n",
       " 'pair': 890,\n",
       " 'fifty': 891,\n",
       " \"there's\": 892,\n",
       " 'raised': 893,\n",
       " 'chin': 894,\n",
       " 'honour': 895,\n",
       " 'state': 896,\n",
       " 'passing': 897,\n",
       " 'purpose': 898,\n",
       " 'subject': 899,\n",
       " 'pretty': 900,\n",
       " 'inquiry': 901,\n",
       " 'investigation': 902,\n",
       " 'ill': 903,\n",
       " 'reach': 904,\n",
       " 'streets': 905,\n",
       " 'law': 906,\n",
       " 'action': 907,\n",
       " 'cigar': 908,\n",
       " 'number': 909,\n",
       " 'knows': 910,\n",
       " 'carry': 911,\n",
       " 'blow': 912,\n",
       " 'draw': 913,\n",
       " 'perfectly': 914,\n",
       " 'precious': 915,\n",
       " 'wonder': 916,\n",
       " 'broken': 917,\n",
       " 'wished': 918,\n",
       " 'possibly': 919,\n",
       " 'narrative': 920,\n",
       " 'real': 921,\n",
       " 'west': 922,\n",
       " 'statement': 923,\n",
       " 'human': 924,\n",
       " \"'no\": 925,\n",
       " 'piece': 926,\n",
       " 'beg': 927,\n",
       " 'lawn': 928,\n",
       " 'shining': 929,\n",
       " 'merryweather': 930,\n",
       " 'iron': 931,\n",
       " 'lens': 932,\n",
       " 'kept': 933,\n",
       " 'weary': 934,\n",
       " 'worn': 935,\n",
       " 'alive': 936,\n",
       " 'firm': 937,\n",
       " 'traces': 938,\n",
       " 'free': 939,\n",
       " 'looks': 940,\n",
       " 'trouble': 941,\n",
       " 'horror': 942,\n",
       " 'sometimes': 943,\n",
       " 'grew': 944,\n",
       " 'opium': 945,\n",
       " 'lascar': 946,\n",
       " 'madam': 947,\n",
       " 'bradstreet': 948,\n",
       " 'orange': 949,\n",
       " 'reasoning': 950,\n",
       " 'drawing': 951,\n",
       " 'throw': 952,\n",
       " 'beyond': 953,\n",
       " 'returning': 954,\n",
       " 'hot': 955,\n",
       " 'post': 956,\n",
       " 'quarter': 957,\n",
       " 'wanted': 958,\n",
       " 'rich': 959,\n",
       " 'seat': 960,\n",
       " 'influence': 961,\n",
       " 'acquaintance': 962,\n",
       " 'opening': 963,\n",
       " 'beautiful': 964,\n",
       " 'handed': 965,\n",
       " 'briony': 966,\n",
       " 'dozen': 967,\n",
       " 'neighbourhood': 968,\n",
       " 'wind': 969,\n",
       " 'search': 970,\n",
       " 'fine': 971,\n",
       " 'contrary': 972,\n",
       " \"woman's\": 973,\n",
       " 'fall': 974,\n",
       " 'creature': 975,\n",
       " 'arm': 976,\n",
       " 'questioning': 977,\n",
       " 'evil': 978,\n",
       " 'value': 979,\n",
       " 'remark': 980,\n",
       " 'thrust': 981,\n",
       " 'fortune': 982,\n",
       " 'learn': 983,\n",
       " 'presume': 984,\n",
       " 'couple': 985,\n",
       " 'ways': 986,\n",
       " 'sake': 987,\n",
       " 'huge': 988,\n",
       " \"'it\": 989,\n",
       " 'exceedingly': 990,\n",
       " \"'yes\": 991,\n",
       " 'lad': 992,\n",
       " 'fingers': 993,\n",
       " 'jones': 994,\n",
       " 'scotland': 995,\n",
       " 'narrow': 996,\n",
       " 'unless': 997,\n",
       " 'curious': 998,\n",
       " 'connection': 999,\n",
       " 'plain': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017337e0",
   "metadata": {
    "id": "017337e0"
   },
   "source": [
    "- In the above code, the text is tokenized, which means it is divided into individual words or tokens. The ‘Tokenizer’ object is created, which will handle the tokenization process.\n",
    "- The ‘fit_on_texts’ method of the tokenizer is called, passing the ‘text’ as input. This method analyzes the text and builds a vocabulary of unique words, assigning each word a numerical index.\n",
    "- The ‘total_words’ variable is then assigned the value of the length of the word index plus one, representing the total number of distinct words in the text.\n",
    "\n",
    "\n",
    "Now let’s create input-output pairs by splitting the text into sequences of tokens and forming n-grams from the sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6806f231",
   "metadata": {
    "id": "6806f231"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in text.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f85a367",
   "metadata": {
    "id": "2f85a367"
   },
   "source": [
    "- In the above code, the text data is split into lines using the ‘\\n’ character as a delimiter.\n",
    "- For each line in the text, the ‘texts_to_sequences’ method of the tokenizer is used to convert the line into a sequence of numerical tokens based on the previously created vocabulary.\n",
    "- The resulting token list is then iterated over using a for loop. For each iteration, a subsequence, or n-gram, of tokens is extracted, ranging from the beginning of the token list up to the current index ‘i’.\n",
    "\n",
    "\n",
    "\n",
    "- This n-gram sequence represents the input context, with the last token being the target or predicted word.\n",
    "- This n-gram sequence is then appended to the ‘input_sequences’ list.\n",
    "- This process is repeated for all lines in the text, generating multiple input-output sequences that will be used for training the next word prediction model.\n",
    "\n",
    "Now let’s pad the input sequences to have equal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648c00e3",
   "metadata": {
    "id": "648c00e3"
   },
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08d05df",
   "metadata": {
    "id": "b08d05df"
   },
   "source": [
    "- In the above code, the input sequences are padded to ensure all sequences have the same length.\n",
    "- The variable ‘max_sequence_len’ is assigned the maximum length among all the input sequences.\n",
    "- The ‘pad_sequences’ function is used to pad or truncate the input sequences to match this maximum length.\n",
    "\n",
    "- The ‘pad_sequences’ function takes the input_sequences list, sets the maximum length to ‘max_sequence_len’, and specifies that the padding should be added at the beginning of each sequence using the ‘padding=pre’ argument.\n",
    "- Finally, the input sequences are converted into a numpy array to facilitate further processing.\n",
    "\n",
    "Now let’s split the sequences into input and output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8cd123f",
   "metadata": {
    "id": "f8cd123f"
   },
   "outputs": [],
   "source": [
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a05490",
   "metadata": {
    "id": "33a05490"
   },
   "source": [
    "- In the above code, the input sequences are split into two arrays, ‘X’ and ‘y’, to create the input and output for training the next word prediction model.\n",
    "- The ‘X’ array is assigned the values of all rows in the ‘input_sequences’ array except for the last column.\n",
    "- It means that ‘X’ contains all the tokens in each sequence except for the last one, representing the input context.\n",
    "\n",
    "\n",
    "- On the other hand, the ‘y’ array is assigned the values of the last column in the ‘input_sequences’ array, which represents the target or predicted word.\n",
    "\n",
    "Now let’s convert the output to one-hot encode vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ec95a6",
   "metadata": {
    "id": "90ec95a6"
   },
   "outputs": [],
   "source": [
    "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d1ce44",
   "metadata": {
    "id": "47d1ce44"
   },
   "source": [
    "- In the above code, we are converting the output array into a suitable format for training a model, where each target word is represented as a binary vector.\n",
    "\n",
    "Now let’s build a neural network architecture to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1140897a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1140897a",
    "outputId": "a0db7775-1eb9-4ebe-b91a-5dd5fe065a0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 39ms/step - accuracy: 0.0589 - loss: 6.6143\n",
      "Epoch 2/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 39ms/step - accuracy: 0.0940 - loss: 5.7634\n",
      "Epoch 3/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 42ms/step - accuracy: 0.1204 - loss: 5.4191\n",
      "Epoch 4/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 39ms/step - accuracy: 0.1365 - loss: 5.1807\n",
      "Epoch 5/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 40ms/step - accuracy: 0.1465 - loss: 4.9725\n",
      "Epoch 6/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 39ms/step - accuracy: 0.1551 - loss: 4.7832\n",
      "Epoch 7/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 46ms/step - accuracy: 0.1616 - loss: 4.6221\n",
      "Epoch 8/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 47ms/step - accuracy: 0.1677 - loss: 4.4655\n",
      "Epoch 9/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 48ms/step - accuracy: 0.1756 - loss: 4.3257\n",
      "Epoch 10/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 49ms/step - accuracy: 0.1831 - loss: 4.1760\n",
      "Epoch 11/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 48ms/step - accuracy: 0.1900 - loss: 4.0506\n",
      "Epoch 12/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 49ms/step - accuracy: 0.2028 - loss: 3.9226\n",
      "Epoch 13/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 48ms/step - accuracy: 0.2121 - loss: 3.8019\n",
      "Epoch 14/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 48ms/step - accuracy: 0.2263 - loss: 3.6933\n",
      "Epoch 15/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 48ms/step - accuracy: 0.2415 - loss: 3.5803\n",
      "Epoch 16/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 47ms/step - accuracy: 0.2521 - loss: 3.4914\n",
      "Epoch 17/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 47ms/step - accuracy: 0.2679 - loss: 3.3875\n",
      "Epoch 18/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 45ms/step - accuracy: 0.2766 - loss: 3.3079\n",
      "Epoch 19/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 39ms/step - accuracy: 0.2895 - loss: 3.2293\n",
      "Epoch 20/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 42ms/step - accuracy: 0.3054 - loss: 3.1461\n",
      "Epoch 21/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 40ms/step - accuracy: 0.3174 - loss: 3.0640\n",
      "Epoch 22/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 41ms/step - accuracy: 0.3292 - loss: 2.9980\n",
      "Epoch 23/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 41ms/step - accuracy: 0.3447 - loss: 2.9194\n",
      "Epoch 24/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 41ms/step - accuracy: 0.3538 - loss: 2.8565\n",
      "Epoch 25/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 41ms/step - accuracy: 0.3673 - loss: 2.7872\n",
      "Epoch 26/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 41ms/step - accuracy: 0.3806 - loss: 2.7185\n",
      "Epoch 27/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 43ms/step - accuracy: 0.3941 - loss: 2.6542\n",
      "Epoch 28/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 41ms/step - accuracy: 0.4049 - loss: 2.5901\n",
      "Epoch 29/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 43ms/step - accuracy: 0.4165 - loss: 2.5393\n",
      "Epoch 30/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 42ms/step - accuracy: 0.4245 - loss: 2.4822\n",
      "Epoch 31/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 41ms/step - accuracy: 0.4364 - loss: 2.4321\n",
      "Epoch 32/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 42ms/step - accuracy: 0.4464 - loss: 2.3719\n",
      "Epoch 33/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 42ms/step - accuracy: 0.4598 - loss: 2.3171\n",
      "Epoch 34/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 41ms/step - accuracy: 0.4676 - loss: 2.2708\n",
      "Epoch 35/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 42ms/step - accuracy: 0.4829 - loss: 2.2122\n",
      "Epoch 36/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 41ms/step - accuracy: 0.4910 - loss: 2.1605\n",
      "Epoch 37/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 42ms/step - accuracy: 0.5026 - loss: 2.1128\n",
      "Epoch 38/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 41ms/step - accuracy: 0.5137 - loss: 2.0626\n",
      "Epoch 39/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 41ms/step - accuracy: 0.5208 - loss: 2.0235\n",
      "Epoch 40/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 41ms/step - accuracy: 0.5320 - loss: 1.9793\n",
      "Epoch 41/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 40ms/step - accuracy: 0.5416 - loss: 1.9369\n",
      "Epoch 42/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 41ms/step - accuracy: 0.5525 - loss: 1.8961\n",
      "Epoch 43/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 41ms/step - accuracy: 0.5595 - loss: 1.8601\n",
      "Epoch 44/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 42ms/step - accuracy: 0.5698 - loss: 1.8151\n",
      "Epoch 45/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 42ms/step - accuracy: 0.5791 - loss: 1.7688\n",
      "Epoch 46/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 41ms/step - accuracy: 0.5861 - loss: 1.7413\n",
      "Epoch 47/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 42ms/step - accuracy: 0.5927 - loss: 1.7050\n",
      "Epoch 48/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 41ms/step - accuracy: 0.5999 - loss: 1.6734\n",
      "Epoch 49/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 42ms/step - accuracy: 0.6080 - loss: 1.6430\n",
      "Epoch 50/50\n",
      "\u001b[1m3010/3010\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 44ms/step - accuracy: 0.6190 - loss: 1.6003\n",
      "<Sequential name=sequential, built=True>\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len - 1))\n",
    "model.add(LSTM(150,return_sequences=True))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(total_words, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "history = model.fit(X, y, epochs=50, verbose=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145b8eea",
   "metadata": {
    "id": "145b8eea"
   },
   "source": [
    "- The code above defines the model architecture for the next word prediction model.\n",
    "- The ‘Sequential’ model is created, which represents a linear stack of layers.\n",
    "- The first layer added to the model is the ‘Embedding’ layer, which is responsible for converting the input sequences into dense vectors of fixed size. It takes three arguments:\n",
    "\n",
    "1. ‘total_words’, which represents the total number of distinct words in the vocabulary;\n",
    "2. ‘100’, which denotes the dimensionality of the word embeddings;\n",
    "3. ‘input_length’, which specifies the length of the input sequences.\n",
    "\n",
    "- The next layer added is the ‘LSTM’ layer, a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data. It has 150 units, which means it will learn 150 internal representations or memory cells.\n",
    "\n",
    "- The next layer added is one more ‘LSTM’ layer, a type of recurrent neural network (RNN) layer designed for capturing sequential dependencies in the data. It has 150 units, which means it will learn 150 internal representations or memory cells.\n",
    "\n",
    "- Finally, the ‘Dense’ layer is added, which is a fully connected layer that produces the output predictions.\n",
    "- It has ‘total_words’ units and uses the ‘softmax’ activation function to convert the predicted scores into probabilities, indicating the likelihood of each word being the next one in the sequence.\n",
    "\n",
    "Now let’s compile and train the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e6ec9c",
   "metadata": {
    "id": "13e6ec9c"
   },
   "source": [
    "\n",
    "- The ‘compile’ method configures the model for training.\n",
    "- The ‘loss’ parameter is set to ‘categorical_crossentropy’, a commonly used loss function for multi-class classification problems.\n",
    "- The ‘optimizer’ parameter is set to ‘adam’, an optimization algorithm that adapts the learning rate during training.\n",
    "\n",
    "\n",
    "- The ‘metrics’ parameter is set to ‘accuracy’ to monitor the accuracy during training.\n",
    "- After compiling the model, the ‘fit’ method is called to train the model on the input sequences ‘X’ and the corresponding output ‘y’.\n",
    "- The ‘epochs’ parameter specifies the number of times the training process will iterate over the entire dataset.\n",
    "- The ‘verbose’ parameter is set to ‘1’ to display the training process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c26a4247",
   "metadata": {
    "id": "c26a4247"
   },
   "outputs": [],
   "source": [
    "def generate_next_words(text, next_words):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "        token_list = pad_sequences([token_list], \n",
    "                                 maxlen=max_sequence_len-1, \n",
    "                                 padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predicted, axis=-1)[0]\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        text += \" \" + output_word\n",
    "    \n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6889ef5b-2472-4873-ad73-e5e013fb009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You would not advise me that you\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"You would\"\n",
    "next_words = 5\n",
    "generated_text = generate_next_words(text, next_words)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
